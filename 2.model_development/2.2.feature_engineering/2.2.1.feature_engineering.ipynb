{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](../../README.md)\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "This Jupyter Notepad is a selection of data engineering processes you can apply to your data before model training to maximise the performance of your machine learning model. For this demonstration we will engineer new or improved features for the diabetes data you previously wrangled.\n",
    "\n",
    "#### Feature Engineering Process\n",
    "- Deriving new variables from existing ones\n",
    "    - Encoding categorical features\n",
    "    - Calculating new features from existing features\n",
    "- Combining features/feature interactions\n",
    "- Identifying the most relevant features for the model\n",
    "- Transforming Features\n",
    "  - [Dividing Data into categories](https://web.ma.utexas.edu/users/mks/statmistakes/dividingcontinuousintocategories.html)\n",
    "  - Mathematical transformations (for example logarithmic transformations). Logarithmic transformations are a powerful tool in the world of statistical analysis. They are often used to transform data that exhibit skewness or other irregularities, making it easier to analyze, visualize, and interpret the results.\n",
    "- Creating Domain-Specific Features that incorporating knowledge from the specific domain to create features that capture important characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import frameworks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Store the data as a local variable\n",
    "\n",
    "The data frame is a Pandas object that structures your tabular data into an appropriate format. It loads the complete data in memory so it is now ready for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"2.2.1.wrangled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Deriving new variables from existing ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding categorical variables\n",
    "\n",
    "Data Encoding converts textual data into numerical format, so that it can be used as input for algorithms to process. The reason for encoding is that most machine learning algorithms work with numbers and not with text or categorical variables.\n",
    "\n",
    "* To encode the 'SEASONS' column we will assign a number value to the season. Because the data set only provides 4 values we will use 1, 2, 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['Seasons'] = data_frame['Seasons'].apply(lambda season: 1 if season.lower() == 'spring' else 2 if season.lower() == 'summer' else 3 if season.lower() == 'fall' else 4 if season.lower() == 'winter' else None)\n",
    "print(data_frame['Seasons'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will do the same for 'FunctioningDay' and 'Holiday' except for 2 values instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['FunctioningDay'] = data_frame['Functioning Day'].apply(lambda day: 1 if day.lower() == 'yes' else 0)\n",
    "print(data_frame['FunctioningDay'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['Holiday'] = data_frame['Holiday'].apply(lambda holiday: 1 if holiday.lower() == 'yes' else 0)\n",
    "print(data_frame['Holiday'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding categorical variables\n",
    "\n",
    "* In the context of urban transportation, rush hour is inbetween the hours of 7-9 (AM) and 5-7 (PM). We will convert two dates, and further encode this into brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime\n",
    "data_frame['Date'] = pd.to_datetime(data_frame['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Create Day of the Week feature\n",
    "data_frame['DayOfWeek'] = data_frame['Date'].dt.dayofweek\n",
    "\n",
    "# Create Rush Hour feature\n",
    "data_frame['RushHour'] = data_frame['Hour'].apply(lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0)\n",
    "\n",
    "# Print the result to verify the new features\n",
    "print(data_frame[['Date', 'Hour', 'DayOfWeek', 'RushHour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining features/feature interactions\n",
    "\n",
    "While individual features can be powerful predictors, their interactions often carry even more information. Feature interaction engineering is the process of creating new features that represent the interaction between two or more features.\n",
    "\n",
    "* In this, case some domain knowledge (urban mobility and transportation) and data analysis have informed us that the more exposure combined with high humidity levels (indicated by high dew point temperatures) can lead to heat-related illnesses such as heat exhaustion or heat stroke. Understanding this interaction can help in predicting lower bike-sharing usage during such conditions to ensure user safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Risk' column\n",
    "data_frame['VSW'] = data_frame['DewPointTemp'] * data_frame['Hour']\n",
    "\n",
    "# Calculate the percentage of the maximum risk\n",
    "data_frame['VSW%'] = (data_frame['VSW'] / data_frame['VSW'].max()).round(2)\n",
    "\n",
    "# Print the result\n",
    "print(data_frame[['DewPointTemp', 'Hour', 'VSW%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Features\n",
    "\n",
    "Filtering is like applying the where clause in a database. It is widely used and can help when you need to work on a specific subset of your data. For our use case, let us filter the data to only include rows where the 'SEASON' is 'Summer'. There is no method call for this, we can just use conditional indexing to fulfil our purpose.\n",
    "\n",
    "* In this, case some domain knowledge and data analysis have informed us that there is 'bimodality' in the data and each season has a different trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to -1 only\n",
    "data_frame = data_frame[data_frame['Seasons'] == 1]\n",
    "\n",
    "# Print the result\n",
    "print(data_frame[['Hour', 'Seasons', 'Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Domain-Specific Features\n",
    "\n",
    "Domain knowledge is about understanding the domain or subject area of the data. In this case, the domain is urban mobility and transportation, which involves understanding how environmental and temporal factors influence bike-sharing demand.\n",
    "\n",
    "The column called Comfort Index is a domain-specific feature as it combines temperature and humidity to reflect how comfortable the weather feels for biking. Domain-specific knowledge indicates that weather comfort significantly impacts bike-sharing usage, as extreme discomfort (e.g., high humidity and temperature) can deter users.\n",
    "\n",
    "* First, we will convert the Comfort Index value to a scaled percentage, because comfort can never be 0 (completely uncomfortable) or 100% (perfectly comfortable). We will scale the result between 0.15 and 0.95 to normalize the values for better interpretability and use in predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Weather Comfort Index (simplified)\n",
    "data_frame['ComfortIndex'] = data_frame['Temp'] - (0.55 * (1 - (data_frame['Humidity'] / 100)) * (data_frame['Temp'] - 14.5))\n",
    "\n",
    "# Scale the ComfortIndex between 0.15 and 0.95\n",
    "min_val = 0.15\n",
    "max_val = 0.95\n",
    "data_frame['ComfortIndexScaled'] = (((data_frame['ComfortIndex'] - data_frame['ComfortIndex'].min()) / (data_frame['ComfortIndex'].max() - data_frame['ComfortIndex'].min())) * (max_val - min_val) + min_val).round(2)\n",
    "\n",
    "# Print the result\n",
    "print(data_frame[['ComfortIndex', 'ComfortIndexScaled', 'Temp', 'Humidity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to make it even more meaningful, we will combine it with the `VSW` feature we engineered using the `Hour` and `DewPointTemp` features to create a combined risk 'interaction feature' that captures real-world relationships between the features.\n",
    "\n",
    "Again we will scale the result between 0.15 and 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['ComfortAdd'] = (data_frame['ComfortIndex'] * data_frame['VSW%']).round(2)\n",
    "\n",
    "min_val = 0.15\n",
    "max_val = 0.85\n",
    "data_frame['ComfortAdd'] = (((data_frame['ComfortAdd'] - data_frame['ComfortAdd'].min()) / (data_frame['ComfortAdd'].max() - data_frame['ComfortAdd'].min())) * (max_val - min_val) + min_val).round(2)\n",
    "\n",
    "# Print the result\n",
    "print(data_frame[['Hour', 'VSW%', 'ComfortIndex', 'ComfortAdd']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the wrangled and engineered data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('../2.3.model_training/2.3.1.model_ready_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
