{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](../../README.md)\n",
    "\n",
    "### Seoul Bike Sharing\n",
    "\n",
    "**Dataset:** https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand\n",
    "\n",
    "We will wrangle the bike data set previewed in the last [Jupyter Notebook](2.model_development/2.1.data_wrangling/2.1.1.data_preview.ipynb).\n",
    "\n",
    "> None of these processes are destructive to the source CSV as long as you save the modified data to a new CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import frameworks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the data as a local variable\n",
    "\n",
    "The data frame is a Pandas object that structures your tabular data into an appropriate format. It loads the complete data in memory so it is now ready for preprocessing. It integrates seamlessly with other data science and machine learning libraries in Python, such as NumPy, SciPy, scikit-learn, and Matplotlib. Additionaly, using a DataFrame makes our code more readble and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"2.1.2.SeoulBikeData_Sample_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with null values\n",
    "\n",
    "Null values during data analysis can cause runtime errors and unexpected results. It is important to identify null values and deal with them appropriately before training a model.\n",
    "\n",
    "The `isnull().sum()` method call returns the null values in any column.\n",
    "\n",
    "- Null data was not captured due to the dataset coming from a reputable source: https://archive.ics.uci.edu\n",
    "\n",
    "    - Additionally, mentioned in the source description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              0\n",
       "Count             0\n",
       "Hour              0\n",
       "Temp              0\n",
       "Humidity          0\n",
       "WindSpeed         0\n",
       "Visibility        0\n",
       "DewPointTemp      0\n",
       "SolarRadiation    0\n",
       "Rainfall          0\n",
       "Snowfall          0\n",
       "Seasons           0\n",
       "Holiday           0\n",
       "FunctioningDay    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we were to detect null values we could remove any row with a null value with a `dropna()` method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              0\n",
       "Count             0\n",
       "Hour              0\n",
       "Temp              0\n",
       "Humidity          0\n",
       "WindSpeed         0\n",
       "Visibility        0\n",
       "DewPointTemp      0\n",
       "SolarRadiation    0\n",
       "Rainfall          0\n",
       "Snowfall          0\n",
       "Seasons           0\n",
       "Holiday           0\n",
       "FunctioningDay    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = data_frame.dropna(subset=['Seasons'])\n",
    "data_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates\n",
    "\n",
    "Duplicate data can have detrimental effects on your machine learning models and outcomes, such as reducing data diversity and representativeness, which can lead to overfitting or biased models.\n",
    "\n",
    "The `duplicated().sum()` method call returns the count of duplicate rows in the data frame.\n",
    "\n",
    "- No duplicates were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we were to find duplicates we could use the drop_duplicates() method call, then stored back onto the data_frame variable removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = data_frame.drop_duplicates()\n",
    "data_frame.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace data\n",
    "\n",
    "We can run a lambda function on a column to modify its values. For a simple example, letâ€™s convert the Seasons to lowercase. To run a function over a complete column, we can use the apply method which iterates over each row and modifies the values.\n",
    "\n",
    "* Replaced upper cases for seasons for better access, **will later be overwritten by binary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    winter\n",
       "1    winter\n",
       "2    winter\n",
       "3    winter\n",
       "4    winter\n",
       "Name: Seasons, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['Seasons'] = data_frame['Seasons'].apply(lambda x: x.lower())\n",
    "data_frame['Seasons'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that there are no data entry errors by the `unique()` method call.\n",
    "\n",
    "-  No data entry errors were caught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['winter', 'spring', 'summer', 'autumn'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['Seasons'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['winter', 'spring', 'summer', 'autumn'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['Seasons'] = data_frame['Seasons'].apply(lambda season: 'winter' if season.lower() == 'winter' else ('summer' if season.lower() == 'summer' else season))\n",
    "data_frame['Seasons'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers\n",
    "\n",
    "Outliers can skew your analysis on numerical columns, and it is important to remove them. We can use the 25th and 75th quartile on numerical data, to get the inter-quartile range. This allows us to estimate an acceptable range, and we can then filter out any values outside this range. Mathematically, outliers are values occurring outside 1.5 times the interquartile range (IQR) from the first quartile (Q1) or third quartile (Q3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8760.000000\n",
      "mean      704.602055\n",
      "std       644.997468\n",
      "min         0.000000\n",
      "25%       191.000000\n",
      "50%       504.500000\n",
      "75%      1065.250000\n",
      "max      3556.000000\n",
      "Name: Count, dtype: float64\n",
      "Outliers are a Count above 2376.625 or below -1120.375\n"
     ]
    }
   ],
   "source": [
    "# Get the inter-quartile range\n",
    "print(data_frame['Count'].describe())\n",
    "Q1 = data_frame['Count'].quantile(0.25)\n",
    "Q3 = data_frame['Count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(f'Outliers are a Count above {Q3 + IQR * 1.5} or below {Q1 - IQR * 1.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8602.000000\n",
      "mean      667.971635\n",
      "std       589.552620\n",
      "min         0.000000\n",
      "25%       188.000000\n",
      "50%       485.000000\n",
      "75%      1030.750000\n",
      "max      2375.000000\n",
      "Name: Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_frame = data_frame[(data_frame['Count'] >= Q1 - 1.5 * IQR) & (data_frame['Count'] <= Q3 + 1.5 * IQR)]\n",
    "print(data_frame['Count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling features to a common range\n",
    "\n",
    "Scaling the features makes it easier for machine learning algorithms to find the optimal solution, as the different scales of the features do not influence them.\n",
    "\n",
    "* I left room for outliers with my `max_count = 2400`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151817/3229296215.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Count_scaled'] = scaler.fit_transform(filtered_df[['Count']])\n"
     ]
    }
   ],
   "source": [
    "# UNTESTED: The following code is not necessary for the task but is included for completeness.\n",
    "# DO NOT USE IF IT DOES NOT WORK\n",
    "\n",
    "Q1 = data_frame['Count'].quantile(0.25)\n",
    "Q3 = data_frame['Count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = data_frame[(data_frame['Count'] >= lower_bound) & (data_frame['Count'] <= upper_bound)]\n",
    "\n",
    "# Now apply MinMaxScaler to 'Count' column\n",
    "scaler = MinMaxScaler()\n",
    "filtered_df['Count_scaled'] = scaler.fit_transform(filtered_df[['Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!important]\n",
    "> You need to save the calculations for each dataset you scale for scaling new values for prediction.\n",
    "> - Calculations were saved for scaling new values for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I also counted how many 0 values there were, due to issues with linear regression (later evident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0 values in each column:\n",
      "Date                 0\n",
      "Count              295\n",
      "Hour               365\n",
      "Temp                21\n",
      "Humidity            17\n",
      "WindSpeed           74\n",
      "Visibility           0\n",
      "DewPointTemp        60\n",
      "SolarRadiation    4300\n",
      "Rainfall          8232\n",
      "Snowfall          8317\n",
      "Seasons              0\n",
      "Holiday              0\n",
      "FunctioningDay       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "zero_counts = data_frame.isin([0]).sum()\n",
    "print(\"Number of 0 values in each column:\")\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rainfall, snowfall and solar radiation appear to have a lot, however these values are in decimals and have little effect\n",
    "\n",
    "* Having 300 Count values as 0 can possibly have issues. However, this may be maintanence days etc.\n",
    "\n",
    "    * Removing these values also decreased test scores by 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the wrangled data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('../2.2.feature_engineering/2.2.1.wrangled_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
